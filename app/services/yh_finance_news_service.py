import asyncio
import httpx
from datetime import datetime
from bs4 import BeautifulSoup
from app.config import logger


async def get_yh_finance_news_urls():
    url_list = []

    try:
        logger.info("ğŸ”” ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì‹œì‘")

        async with httpx.AsyncClient(follow_redirects=True) as client:
            url = "https://finance.yahoo.com"
            headers = {
                "User-Agent": (
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/91.0.4472.124 Safari/537.36"
                )
            }

            logger.info(f"ğŸ“ ìš”ì²­ URL: {url}")
            response = await client.get(url, headers=headers)
            if response.status_code != 200:
                logger.error(f"âŒ ìš”ì²­ ì‹¤íŒ¨: HTTP {response.status_code}")
                return url_list

            soup = BeautifulSoup(response.content, "html.parser")
            news_list = soup.select(
                ".hero-latest-news .story-items .story-item .container .content"
            )
            logger.debug(f"ğŸ“ {len(news_list)}ê°œì˜ ë‰´ìŠ¤ í•­ëª© ë°œê²¬")

            for line in news_list:
                a_tag = line.find("a")
                if a_tag:
                    news_url = a_tag.get("href")
                    url_list.append(news_url)

        logger.info(f"âœ… ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì™„ë£Œ - ì´ {len(url_list)}ê°œì˜ URL ìˆ˜ì§‘ë¨")
        return url_list

    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return url_list



# ë‰´ìŠ¤ URL ë‚´ë¶€ì˜ ìƒì„¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
async def get_yh_finance_news_content(url: str, client: httpx.AsyncClient):
    contents_list = []  # ê° ë¬¸ë‹¨ì˜ í…ìŠ¤íŠ¸ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/91.0.4472.124 Safari/537.36"
        )
    }
    try:
        logger.debug(f"ğŸ“ ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© ìš”ì²­: {url}")
        response = await client.get(url, headers=headers)
        if response.status_code != 200:
            logger.error(
                f"âŒ ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© ìš”ì²­ ì‹¤íŒ¨: {url} - HTTP {response.status_code}"
            )
            return None

        soup = BeautifulSoup(response.content, "html.parser")
        title_tag = soup.select_one(".cover-title")
        title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

        article_p_tags = soup.select(".atoms-wrapper > p")
        for p_tag in article_p_tags:
            text = p_tag.get_text(strip=True)
            if text:
                contents_list.append(text)

        joined_content = ", ".join(contents_list)

        news_data = {
            "url": url,
            "title": title,
            "content": joined_content,
        }
        logger.debug(f"ğŸ“ ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {url}")
        return news_data

    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ ë‚´ìš© ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {url} - {e}")
        return None


async def get_yh_finance_news_contents():
    news_urls = await get_yh_finance_news_urls()
    news_contents = []
    async with httpx.AsyncClient() as client:
        tasks = [get_yh_finance_news_content(url, client) for url in news_urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for result in results:
            if isinstance(result, Exception):
                logger.error(
                    f"âŒ get_yh_finance_news_content ì‘ì—…ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {result}"
                )
            elif result:
                news_contents.append(result)
    logger.info(
        f"âœ… ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘ ì™„ë£Œ - ì´ {len(news_contents)}ê°œì˜ ë‰´ìŠ¤ ë‚´ìš© ìˆ˜ì§‘ë¨"
    )
    return news_contents
